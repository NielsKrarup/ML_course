{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Christian Igel, 2023\n",
    "\n",
    "\n",
    "I took inspiration from https://github.com/tirthajyoti/Interactive_Machine_Learning by Tirthajyoti Sarkar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, IntSlider, Layout, interact_manual, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit\n",
    "Just some artificial function with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_gen(N_samples,noise_sd, resolution=500):\n",
    "    def func(x):\n",
    "        return 2*x-0.6*x**2+0.2*x**3+12*np.sin(x)\n",
    "    x_min=-5\n",
    "    x_max=5\n",
    "    x1= np.linspace(x_min,x_max,resolution)  # equally spaced points\n",
    "    x= np.random.choice(x1,size=N_samples)  # random points\n",
    "    y=func(x)\n",
    "    y1=func(x1)\n",
    "    y= y+np.random.normal(scale=noise_sd,size=N_samples)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x1,y1,c='k',lw=2)\n",
    "    plt.scatter(x,y,edgecolors='k',c='orange',s=60)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (x.reshape(-1,1),y,x1,y1,x_min,x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the interactive widget with the data generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p=interactive(func_gen, \n",
    "              N_samples=widgets.IntSlider(min=10,max=200.,step=10,continuous_update=False,value=100),\n",
    "              #x_min=(-5,0,1), x_max=(0,5,1),\n",
    "              noise_sd=widgets.FloatSlider(min=0.,max=10.,step=0.5,continuous_update=False,value=2.5),resolution=fixed(500))\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the plot\n",
    "x,y,x1,y1,x_min,x_max = p.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models encapsulated in a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_eps = 0.01\n",
    "lasso_nalpha = 20  # alpha = lambda \n",
    "lasso_iter=100000\n",
    "\n",
    "def func_fit(model_type,test_size,degree,alpha,resolution=500):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=test_size,random_state=55)\n",
    "    \n",
    "    if (model_type=='Linear regression'):\n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                          LinearRegression(fit_intercept=False))\n",
    "    if (model_type=='Ridge'):    \n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                              Ridge(alpha=np.exp(alpha),fit_intercept=False,solver='svd'))\n",
    "    if (model_type=='LASSO'):    \n",
    "        model = make_pipeline(PolynomialFeatures(degree,interaction_only=False), \n",
    "                              Lasso(alpha=np.exp(alpha),fit_intercept=False,max_iter=lasso_iter, tol=0.0001, warm_start=True, positive=False, selection='random'))\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    train_pred = np.array(model.predict(X_train))\n",
    "    train_score = model.score(X_train,y_train)\n",
    "    \n",
    "    test_pred = np.array(model.predict(X_test))\n",
    "    test_score = model.score(X_test,y_test)\n",
    "    \n",
    "    X_grid = np.linspace(x_min, x_max, resolution).reshape(-1,1)\n",
    "    y_grid = np.array(model.predict(X_grid))\n",
    "    \n",
    "    plt.figure(figsize=(14,6))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Test set performance\\nTest score: %.3f\"%(test_score))\n",
    "    plt.xlabel(\"X-test\")\n",
    "    plt.ylabel(\"y-test\")\n",
    "    plt.scatter(X_test,y_test,c='blue',s=60,label='Actual test values')\n",
    "    plt.scatter(X_test,test_pred,c='orange',s=40,label='Predicted values')\n",
    "    plt.plot(X_grid, y_grid, label=\"Model\",c='k',lw=2)\n",
    "    y_min = np.min([y_test.min(), test_pred.min()])\n",
    "    y_max = np.min([y_test.max(), test_pred.max()])\n",
    "    plt.ylim([np.amin([1.1*y_min, 0.9*y_min]), np.amax([1.1*y_max, 0.9*y_max])])\n",
    "    plt.plot(x1, y1, label=\"Function w/o noise\",c='red',lw=2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Training set performance\\nTraining score: %.3f\"%(train_score))\n",
    "    plt.xlabel(\"X-train\")\n",
    "    plt.ylabel(\"y-train\")\n",
    "    plt.scatter(X_train,y_train,c='blue', label=\"Traning data\", s=60)\n",
    "    plt.scatter(X_train,train_pred,c='orange', label=\"Fitted values\", s=40)\n",
    "    plt.plot(X_grid, y_grid, label=\"Model\",c='k',lw=2)\n",
    "    y_min = np.min([y_train.min(), train_pred.min()])\n",
    "    y_max = np.min([y_train.max(), train_pred.max()])\n",
    "    plt.ylim([np.amin([1.1*y_min, 0.9*y_min]), np.amax([1.1*y_max, 0.9*y_max])])\n",
    "    plt.plot(x1, y1, label=\"Function w/o noise\",c='red',lw=2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "       \n",
    "    return (train_score,test_score,model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the encapsulated ML function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "# Continuous_update = False for IntSlider control to stop continuous model evaluation while the slider is being dragged\n",
    "m = interactive(func_fit,model_type=widgets.RadioButtons(options=['Linear regression','LASSO', 'Ridge'],\n",
    "                                                    description = \"Choose Model\",style=style,\n",
    "                                                        layout=Layout(width='250px')),\n",
    "                test_size=widgets.Dropdown(options={\"10% of data\":0.1,\"20% of data\":0.2, \"30% of data\":0.3,\n",
    "                                                    \"40% of data\":0.4,\"50% of data\":0.5},\n",
    "                                          description=\"Test set size\",style=style, value=0.3),\n",
    "                degree=widgets.IntSlider(min=1,max=40,step=1,description= 'Polynomial ($\\sum_{i=0}^n w_i x^i$)',\n",
    "                                       style=style,continuous_update=False),\n",
    "                alpha=widgets.FloatSlider(min=-15,max=10,step=1,description= '$\\ln\\lambda$',\n",
    "                                       style=style,continuous_update=False),\n",
    "                resolution=fixed(500))\n",
    "\n",
    "# Display the control\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_train, e_text, model = m.result\n",
    "print(model[1].coef_)\n",
    "\n",
    "plt.xlabel('Parameter number (0 is intercept)')\n",
    "plt.ylabel('Parameter value')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(np.arange(0, len(model[1].coef_)))\n",
    "plt.bar(np.arange(0, len(model[1].coef_)), model[1].coef_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Set parameters and parameter ranges, which should depend on whether the model type is LASSO or Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .30  # percentage of data useds as (external) test set\n",
    "log_lambda_range = np.arange(-5,3)  # range of regularization parameters on logarithic scale\n",
    "degree_range = np.arange(1,6)  # range of degrees for polynomial representation\n",
    "folds = 5  # number of folds for cross-validation\n",
    "model_type = 'LASSO'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the cross-validation. The regularization parameter $\\gamma$ is referred to as ``lambda`` in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = 0\n",
    "best_log_lambda = 0\n",
    "best_cv_score = 0\n",
    "\n",
    "# (X_train, y_train) is used for cross-validation\n",
    "# (X_test, y_test) is used for testing the model and is not used in the training and model selecdtion (cross-validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size,random_state=55)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for degree in degree_range:  # loop over degrees\n",
    "    # Compute polynomial features including constant (bias) feature (include_bias=True)\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    for log_lambda in log_lambda_range:  # loop of regularization parameters\n",
    "        cv_score = 0\n",
    "        for train_i, val_i in kf.split(X_train_poly):  # loop over cross-validation folds\n",
    "            X_train_fold, y_train_fold = X_train_poly[train_i], y_train[train_i]\n",
    "            X_val_fold, y_val_fold = X_train_poly[val_i], y_train[val_i]\n",
    "    \n",
    "            # Only the current training data is used for scaling\n",
    "            scaler.fit_transform(X_train_fold)\n",
    "            scaler.transform(X_val_fold)\n",
    "     \n",
    "            # Features include constant (bias), so we need not fit an extra intercept \n",
    "            if (model_type=='Ridge'):\n",
    "                model = Ridge(alpha=np.exp(log_lambda), fit_intercept=False)\n",
    "            elif (model_type=='LASSO'):\n",
    "                model = Lasso(alpha=np.exp(log_lambda), fit_intercept=False, max_iter=50000)\n",
    "            else:\n",
    "                print(\"unknown model\")\n",
    "                assert(False)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "            score = model.score(X_val_fold, y_val_fold)\n",
    "            cv_score += score\n",
    "        if cv_score > best_cv_score:  # found new best score? \n",
    "            best_cv_score = cv_score\n",
    "            best_degree = degree\n",
    "            best_log_lambda = log_lambda\n",
    "            \n",
    "print(\"Best average CV score:\", best_cv_score / folds, \"degree:\", best_degree, \"log. gamma:\", best_log_lambda)\n",
    "\n",
    "# After cross-validation, the model \n",
    "# is trained with the best parameters on the full training data set and \n",
    "# evaluated on the external test data\n",
    "poly = PolynomialFeatures(degree=best_degree, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly  = poly.transform(X_test)\n",
    "scaler.fit_transform(X_train_poly)\n",
    "scaler.transform(X_test_poly)\n",
    "if (model_type=='Ridge'):\n",
    "    model = Ridge(alpha=np.exp(best_log_lambda),fit_intercept=False,solver='svd')\n",
    "elif (model_type=='LASSO'):\n",
    "    model = Lasso(alpha=np.exp(best_log_lambda),fit_intercept=False)\n",
    "else:\n",
    "    print(\"unknown model\")\n",
    "    assert(False)\n",
    "model.fit(X_train_poly, y_train)\n",
    "score = model.score(X_test_poly, y_test)\n",
    "\n",
    "print(\"Performance best model on test set:\", score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
